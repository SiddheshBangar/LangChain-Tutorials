{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts\n",
    "\n",
    "In Langchain, a prompt is a text string that is used to control the behavior of a language model. Prompts can be used to provide instructions to the model, to provide examples to help the model generate a better response, or to ask the model a question.\n",
    "\n",
    "In Python, prompts can be created using the `PromptTemplate` class. The `PromptTemplate` class takes a template string as input, and it also takes a list of input variables. The input variables are used to replace the placeholders in the template string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=openai_api_key)\n",
    "\n",
    "prompt = \"\"\"\n",
    "Today's date is 21st of April and tommorow is 23rd April\n",
    "\n",
    "What is wrong can you help me out?\n",
    "\"\"\"\n",
    "\n",
    "llm(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompts are a powerful way to control the behavior of language models. By using prompts, you can train language models to do a variety of tasks, such as translating languages, writing different kinds of creative text, and answering your questions in an informative way."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci=003\", openai_api_key=openai_api_key)\n",
    "\n",
    "template = \"I desperately want to travel the most iconic places in {location} can you suggest some good places there?\"\n",
    "\n",
    "prompt = PromptTemplate(template, input_variables=[\"location\"], template=template)\n",
    "\n",
    "final_prompt = prompt.format(location=\"India\")\n",
    "\n",
    "print(f\"Final Prompt: {final_prompt}\")\n",
    "print(f\"LLM Output: {llm(final_prompt)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code first imports the `OpenAI` and `PromptTemplate` classes from the `langchain` library. Then, it creates an OpenAI LLM object with the `text-davinci-003` model and the API key. Next, it creates a prompt template with the following text:\n",
    "\n",
    "\n",
    "I desperately want to travel the most iconic places in {location} can you suggest some good places there?\n",
    "\n",
    "\n",
    "The template uses the placeholder `{location}` to represent the location that the user wants to travel to. The code then creates a prompt object from the template and the location input variable. The location input variable is set to \"India\". Finally, the code prints the final prompt and the LLM output.\n",
    "\n",
    "The LLM output is a list of text strings that suggest some good places to travel to in India. The LLM output is generated by using the `text-davinci-003` model to complete the prompt. The model is trained on a massive dataset of text and code, so it is able to generate text that is relevant to the prompt and that is grammatically correct.\n",
    "\n",
    "The code in this example shows how to use the `PromptTemplate` class to create a prompt that can be used to generate text with the help of an LLM. The code also shows how to use the `OpenAI` class to create an OpenAI LLM object and to use the object to generate text."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Example Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import faiss\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=openai_api_key)\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Example Input: {input}\\n Example Output: {output}\",\n",
    ")\n",
    "\n",
    "# Examples of locations that nouns are found\n",
    "\n",
    "examples = [\n",
    "    {\"input\": \"spaceship\", \"output\": \"space\"},\n",
    "    {\"input\": \"Social Media\", \"output\": \"Twitter\"}\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It creates a prompt template with the following text:\n",
    "Example Input: {input}\n",
    "Example Output: {output}\n",
    "\n",
    "The template uses the placeholders `{input}` and `{output}` to represent the input and output of the prompt. The code then creates a list of examples that show the relationship between the input and output of the prompt. The examples are in the form of dictionaries, where the `input` key is the input to the prompt and the `output` key is the output of the prompt.\n",
    "\n",
    "The code then uses the `SemanticSimilarityExampleSelector` class to select examples that are similar to the input. The `SemanticSimilarityExampleSelector` class uses the `OpenAIEmbeddings` class to calculate the semantic similarity between the input and the examples. The `faiss` library is used to perform the similarity search.\n",
    "\n",
    "The output of the code is a list of examples that are similar to the input. The list of examples can be used to improve the performance of the prompt. For example, the prompt can be modified to include the examples in the prompt template. This will help the LLM to generate a better response to the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SemanticSimilarityExampleSelector will select examples that are similar to your input\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples,\n",
    "    OpenAIEmbeddings(openai_api_key=openai_api_key),\n",
    "    faiss,\n",
    "    k=2\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code first imports the `SemanticSimilarityExampleSelector` class from the `langchain` library. Then, it creates a `SemanticSimilarityExampleSelector` object with the following parameters:\n",
    "\n",
    "\n",
    "* `examples`: The list of examples that are used to train the example selector.\n",
    "* `embeddings`: The embeddings that are used to calculate the semantic similarity between the examples and the input.\n",
    "* `index`: The faiss index that is used to perform the similarity search.\n",
    "* `k`: The number of examples that are returned by the example selector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Give the location an item us usually found in\",\n",
    "    suffix=\"Input: {noun}\\nOutput:\",\n",
    "    input_variables=[\"noun\"],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code first imports the `FewShotPromptTemplate` class from the `langchain` library. Then, it creates a `FewShotPromptTemplate` object with the following parameters:\n",
    "\n",
    "\n",
    "* `example_selector`: The example selector that is used to select examples that are similar to the input.\n",
    "* `example_prompt`: The prompt template that is used to format the examples.\n",
    "* `prefix`: The prefix that is used to start the prompt.\n",
    "* `suffix`: The suffix that is used to end the prompt.\n",
    "* `input_variables`: The list of input variables that are used in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_noun = \"College\"\n",
    "\n",
    "print(similar_prompt.format(noun=my_noun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(similar_prompt.format(noun=my_noun))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_schema = [\n",
    "    ResponseSchema(name=\"negative-string\", description=\"This is a negative response string\"),\n",
    "    ResponseSchema(name=\"positive-string\", description=\"This is a positive response string\")\n",
    "]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schema)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code first imports the `StructuredOutputParser` class from the `langchain` library. Then, it creates a `StructuredOutputParser` object with the following parameters:\n",
    "\n",
    "* `response_schema`: The list of response schemas that are used to parse the output of the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(output_parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"\"\"\n",
    "You will be given a negative response from a user\n",
    "Rephrase it into a positive one\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "% USER INPUT:\n",
    "{user_input}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"user_input\"]\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    "    template=template\n",
    ")\n",
    "\n",
    "promptValue = prompt.format(user_input=\"Welcome to India\")\n",
    "\n",
    "print(promptValue)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code first defines a prompt template that contains the following elements:\n",
    "\n",
    "* A header that explains the task to the user.\n",
    "* A placeholder for the user's input.\n",
    "* A placeholder for the language model's response.\n",
    "\n",
    "The code then creates a `PromptTemplate` object from the prompt template. The `PromptTemplate` object is used to format the prompt and to replace the placeholders with the user's input.\n",
    "\n",
    "The code then formats the prompt with the user's input. The output of the code is the following prompt:\n",
    "\n",
    "```\n",
    "You will be given a negative response from a user\n",
    "Rephrase it into a positive one\n",
    "\n",
    "% USER INPUT:\n",
    "Welcome to India\n",
    "\n",
    "YOUR RESPONSE:\n",
    "```\n",
    "\n",
    "The prompt asks the language model to rephrase the negative response \"Welcome to India\" into a positive one. The language model can then generate a positive response, such as \"I'm glad you're here!\".\n",
    "\n",
    "The code also defines a variable called `format_instructions`. This variable is used to format the instructions for the user. The default value of the variable is \"Here are the instructions: \". However, you can change the value of the variable to anything you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_output = llm(promptValue)\n",
    "llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser.parse(llm_output)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
