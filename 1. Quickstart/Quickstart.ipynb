{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“• Quickstart Help\n",
    "\n",
    "This notebook will provide a quickstart on the usage of Langchain Library using Python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘‡ Use this below lines to download the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install openai --quiet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘‡ Add your OpenAI API key in the below code block to execute the prompts using OpenAI machine.\n",
    "\n",
    "ðŸ”— OpenAI site: https://openai.com/\n",
    "\n",
    "ðŸ“Œ Make sure that you have a paid account in the OpenAI site so that you can get free trail usage of the api key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"OPENAI_API_KEY\"]=\"your-api-key\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying using OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code line creates a new object called `llm` of the OpenAI class. The OpenAI class is a Python class that provides access to the OpenAI GPT-3 language model. The temperature parameter controls how random the output of the language model is. A `temperature` of `0` means that the language model will always choose the most likely word, while a temperature of `1` means that the language model will choose words with equal probability. A `temperature` of `0.9` means that the language model will choose words with a slightly lower probability, which will result in more variation and creativity in the output.\n",
    "\n",
    "In other words, the code line `llm = OpenAI(temperature=0.9)` creates a new language model object with a temperature of 0.9. This means that the language model will be more likely to generate creative and unexpected output.\n",
    "\n",
    "Here is a breakdown of the code line:\n",
    "- llm is the name of the new object.\n",
    "- OpenAI is the name of the class that the object belongs to.\n",
    "- temperature=0.9 is a parameter that controls the randomness of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"What are 5 vacation destinations for someone who likes to eat pizza?\"\n",
    "print(llm(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* The code line defines a variable `text` and assigns it the value of the string \"What are 5 vacation destinations for someone who likes to eat pizza?\".\n",
    "* The code line then calls the function `llm()` and passes the variable `text` as an argument.\n",
    "* The `llm()` function is a custom function that you have created. It takes a string as an input and returns a list of 5 vacation destinations for someone who likes to eat pizza.\n",
    "* The code line then prints the output of the `llm()` function.\n",
    "\n",
    "In other words, the code line asks the `llm()` function to return a list of 5 vacation destinations for someone who likes to eat pizza, and then prints the output of the `llm()` function.\n",
    "\n",
    "Here is an example of the output of the `llm()` function:\n",
    "\n",
    "```python\n",
    "[\n",
    "  \"New York City\",\n",
    "  \"Naples, Italy\",\n",
    "  \"Chicago, Illinois\",\n",
    "  \"Los Angeles, California\",\n",
    "  \"Toronto, Canada\"\n",
    "]\n",
    "```\n",
    "\n",
    "As you can see, the `llm()` function returns a list of 5 popular vacation destinations for people who like to eat pizza. The list includes cities in the United States, Italy, and Canada."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"food\"],\n",
    "    template=\"What are 5 vacation destinations for someone who likes to eat {food}?\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* `PromptTemplate` is a class from the `transformers` library. It is used to create a prompt that can be used to generate text.\n",
    "* The `input_variables` argument is a list of the names of the variables that will be used in the prompt. In this case, the only variable is `food`.\n",
    "* The `template` argument is the text of the prompt. The `{food}` placeholder will be replaced with the value of the `food` variable when the prompt is used to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are 5 vacation destinations for someone who likes to eat dessert?\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(food=\"dessert\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm(prompt.format(food=\"dessert\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"sports\"],\n",
    "    template=\"Top 5 {sports} players of all time\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code line `llm = OpenAI(temperature=0.7)` creates an instance of the `OpenAI` class with a temperature of 0.7. The temperature parameter controls how random the language model's output is. A temperature of 0.7 means that the model will be more creative and generate more unexpected results.\n",
    "\n",
    "The `PromptTemplate` class is used to create a template that the language model will use to generate text. The `input_variables` parameter specifies the variables that will be used in the template. In this case, the only input variable is `sports`, which will be replaced with the name of a sport. The `template` parameter specifies the text that will be generated by the language model. In this case, the template will generate a list of the top 5 players of a given sport.\n",
    "\n",
    "To use the code, you would first need to create an instance of the `OpenAI` class with a desired temperature. Then, you would create a `PromptTemplate` object with the desired input variables and template. Finally, you would call the `generate` method on the `PromptTemplate` object to generate the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.run(\"football\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: google-search-results is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install google-search-results --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"SERPAPI_API_KEY\"]=\"\"\n",
    "\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code line sets the environment variable `SERPAPI_API_KEY` to an empty string and then calls the `load_tools()` function to return a list of tools that can be used by the agent. The list of tools includes the serpapi tool, which uses the SerpAPI search engine, and the `llm-math` tool, which uses a large language model to answer math questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(tools, llm, agent=\"zero-shor-react-description\", verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code line `agent = initialize_agent(tools, llm, agent=\"zero-shor-react-description\", verbose=True)` initializes an agent using the `initialize_agent()` function from the `langchain.agents` module. The `initialize_agent()` function takes three arguments:\n",
    "\n",
    "* `tools`: A list of tools that the agent will be able to use.\n",
    "* `llm`: An LLM that the agent will use to generate text.\n",
    "* `agent`: The type of agent to initialize. In this case, the agent type is `\"zero-shor-react-description\"`, which is a type of agent that uses the ReAct framework to decide which tool to use, based solely on the tool's description.\n",
    "* `verbose`: A boolean value that indicates whether or not the agent should print out its output. In this case, the verbose flag is set to `True`, so the agent will print out its output.\n",
    "\n",
    "The `initialize_agent()` function returns an agent object. This object can then be used to interact with the tools and the LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\"Who is the current winner of the UEFA Champions League? Who is the oldest member of their team?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "conversation = ConversationChain(llm=llm, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Line 1:** `llm = OpenAI(temperature=0)`\n",
    "\n",
    "This line of code creates an instance of the `OpenAI` class from the `langchain` library. The `OpenAI` class is used to interact with OpenAI's GPT-3 language model. The `temperature` parameter controls the randomness of the model's output. A temperature of `0` means that the model will be very deterministic, and will always output the same text for a given prompt.\n",
    "\n",
    "**Line 2:** `conversation = ConversationChain(llm=llm, verbose=True)`\n",
    "\n",
    "This line of code creates an instance of the `ConversationChain` class. The `ConversationChain` class is used to chain together multiple prompts and responses from the GPT-3 model. The `llm` parameter specifies the `OpenAI` instance that will be used to generate the responses. The `verbose` parameter specifies whether or not the `ConversationChain` class should print out a log of the prompts and responses that it generates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"I want to talk to you regarding sports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Can we talk about football?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Which is your favourite attacking team?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
